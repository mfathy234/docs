# Questions Bank

<table>
  <tr style="background-color: #f2f2f2;">
    <th>Briefly describe your QA background and projects.</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Manual testing experience</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Defect tracking and reporting (Jira, TestRail, etc.)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Collaboration with developers and product team</td>
    <td>1</td>
  </tr>
  <tr>
    <td>End-to-end QA lifecycle involvement</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Exposure to multiple project domains (ERP, web, mobile)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Understanding of SDLC/STLC</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Writing and executing test cases</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Experience with regression or UAT testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Clear explanation of QA role and contributions</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Measurable QA impact (e.g., reduced defects, improved coverage)</td>
    <td>1</td>
  </tr>
</table>

<table>
  <tr style="background-color: #f2f2f2;">
    <th>Types of testing you’ve done?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Functional testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Regression testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Smoke / Sanity testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Integration testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>User Acceptance Testing (UAT)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Exploratory testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Performance / Load testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>API testing (Postman, Swagger, etc.)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Compatibility / Cross-browser testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Security or Negative testing</td>
    <td>1</td>
  </tr>
</table>

<table>
  <tr style="background-color: #f2f2f2;">
    <th>How you keep your QA skills updated?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Regularly learning from QA blogs or YouTube tutorials</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Following QA communities or forums (e.g., Ministry of Testing, Reddit QA)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Taking online courses or certifications (ISTQB, Udemy, Coursera)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Staying updated with testing tools and trends (e.g., Postman, JMeter, Cypress)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Practicing new test management or automation tools</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Learning from peer reviews and retrospectives</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Participating in webinars or QA conferences</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Reading QA or Agile-related documentation (ISTQB, ISO, Agile testing guides)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Experimenting with side projects or open-source QA tools</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Networking and knowledge sharing with QA peers</td>
    <td>1</td>
  </tr>
</table>

<table>
  <tr style="background-color: #f2f2f2;">
    <th>Difference between regression and retesting.</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Regression testing checks for new defects after changes or fixes</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Retesting verifies specific defects that were previously fixed</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Regression ensures no side effects from new code changes</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Retesting confirms that the defect is actually resolved</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Regression covers unchanged areas as well</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Retesting is limited to failed test cases only</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Regression is part of maintenance testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Retesting is a confirmation testing activity</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Regression can be automated</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Retesting is usually done manually</td>
    <td>1</td>
  </tr>
</table>

<table>
  <tr style="background-color: #f2f2f2;">
    <th>Test case design techniques?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Equivalence Partitioning</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Boundary Value Analysis (BVA)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Decision Table Testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>State Transition Testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Use Case Testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Error Guessing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Exploratory Testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Pairwise / Combinatorial Testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Cause-Effect Graphing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Checklist-Based Testing</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>Severity vs priority difference?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Severity measures the impact of a defect on the system</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Priority defines the order in which defects should be fixed</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Severity is decided by QA based on functionality impact</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Priority is decided by project manager or product owner</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Severity shows the technical seriousness of a bug</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Priority shows the business urgency to fix the bug</td>
    <td>1</td>
  </tr>
  <tr>
    <td>High severity, low priority example: crash in rarely used module</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Low severity, high priority example: typo on home page</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Severity remains same across builds; priority can change</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Both help determine defect triage and release readiness</td>
    <td>1</td>
  </tr>
</table>

<table>
  <tr style="background-color: #f2f2f2;">
    <th>What’s in a good test case?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Clear and descriptive test case title</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Unique test case ID or reference number</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Preconditions or setup steps defined</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Well-defined test steps in logical order</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Expected results clearly stated</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Actual results section for execution outcome</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Test data specified where applicable</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Pass/fail criteria clearly defined</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Linked requirement or user story reference</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Written in simple, unambiguous language</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>What is exploratory testing?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Testing without predefined test cases or scripts</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Simultaneous learning, test design, and execution</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Focuses on discovering unknown or unexpected defects</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Tester uses creativity, intuition, and domain knowledge</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Helps identify usability and edge-case issues</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Often used when documentation is limited or incomplete</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Session-based approach with defined time and goals</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Encourages tester ownership and critical thinking</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Finds issues missed by scripted or automated tests</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Documented through notes, mind maps, or session reports</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>Describe sanity vs smoke testing.</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Smoke testing checks basic functionality after build deployment</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Sanity testing verifies specific bug fixes or new functionality</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Smoke testing is broad and shallow</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Sanity testing is narrow and deep</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Smoke testing ensures the system is stable enough for further testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Sanity testing ensures recent changes didn’t break specific areas</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Smoke testing is usually planned and documented</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Sanity testing is often unscripted and quickly executed</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Smoke test is performed on new builds</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Sanity test is performed after minor code or defect fixes</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>Common QA metrics you track?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Defect density (defects per module or size)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Test coverage (percentage of requirements or code tested)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Defect leakage (bugs found after release)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Test execution progress (executed vs planned)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Defect rejection rate (invalid or duplicate bugs)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Automation coverage (percentage of tests automated)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Defect severity and priority distribution</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Mean time to detect (MTTD) and Mean time to fix (MTTF)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Pass/fail rate of test cases</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Customer-found defect count or UAT defect count</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>What is test plan content?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Test plan identifier and version</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Scope and objectives of testing</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Features to be tested and not tested</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Test strategy and approach (manual/automation)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Test environment and configuration details</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Roles and responsibilities of team members</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Entry and exit criteria for testing phases</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Test deliverables (reports, logs, summaries)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Testing schedule and milestones</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Risk assessment and mitigation plan</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>What key metrics do you measure in performance testing?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Response time (average and percentile)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Throughput (requests or transactions per second)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Latency (delay between request and response)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Concurrent users or load (active sessions)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Error rate (failed transactions or HTTP errors)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>CPU utilization</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Memory usage</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Network bandwidth or throughput consumption</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Disk I/O performance</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Peak response time and system bottlenecks</td>
    <td>1</td>
  </tr>
</table>

<table>
  <tr style="background-color: #f2f2f2;">
    <th>What’s the difference between load testing and stress testing?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Load testing checks performance under expected user load</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Stress testing checks system behavior beyond maximum capacity</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Load testing focuses on stability and throughput</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Stress testing focuses on system failure and recovery</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Load testing identifies performance bottlenecks under normal usage</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Stress testing identifies the system’s breaking point</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Load testing ensures acceptable response time under expected load</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Stress testing ensures system stability under extreme conditions</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Load testing is part of performance tuning</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Stress testing is part of reliability and failure analysis</td>
    <td>1</td>
  </tr>
</table>
<table>
  <tr style="background-color: #f2f2f2;">
    <th>How do you analyze performance test results?</th>
    <th>10</th>
  </tr>
  <tr>
    <td>Compare actual results with baseline or SLA targets</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Identify performance bottlenecks (CPU, memory, DB, network)</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Analyze response time trends and percentile distributions</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Review throughput, error rate, and concurrency patterns</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Correlate metrics from application, server, and database logs</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Identify the breaking point or saturation level</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Monitor resource utilization across components</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Document observations, anomalies, and trends</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Recommend optimization or tuning actions</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Prepare and share summary report with key findings and graphs</td>
    <td>1</td>
  </tr>
</table>
